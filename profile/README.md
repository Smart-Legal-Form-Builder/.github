# Smart Legal Form Builder: AI-Enhanced Document Creation
> LawReady is smart legal form builder for everyone. 
Create legal documents and get personalized legal advice effortlessly with LawReady!


![lawready-001 (1)](https://github.com/user-attachments/assets/a76d3d53-092e-4db0-9685-36526923e3ac)



## ğŸ¬ Demo video! (click below)
ğŸ”— https://www.youtube.com/watch?v=zhuRicXTcVs




## ğŸ“¢ Group Members

| Name         | Organization                                              | Email                        | Role             |
|--------------|-----------------------------------------------------------|------------------------------|------------------|
| Seoyeon Kim  | Department of Tourism (ê´€ê´‘í•™ë¶€), Hanyang University      | soen0814@hanyang.ac.kr       | AI / AI backend               |
| Jiwon Han    | Department of Business Administration (ê²½ì˜í•™ë¶€), Hanyang University | jiwon8623@hanyang.ac.kr      | Document         |
| Taeyeon Kim  | Department of Information System (ì •ë³´ì‹œìŠ¤í…œí•™ê³¼), Hanyang University | tangboll5075@gmail.com       | Frontend         |
| Suhyeon Yu   | Department of Information System (ì •ë³´ì‹œìŠ¤í…œí•™ê³¼), Hanyang University | sh265826@naver.com           | Frontend/Backend |
| Soohan Jeong | Department of Information System (ì •ë³´ì‹œìŠ¤í…œí•™ê³¼), Hanyang University | soohan9909@gmail.com         | UI Design        |


---

# Project Overview

This project initially aimed to develop a service that automatically generates legal complaint documents using AI. However, it became clear that establishing a consistent template format was more efficient. As a result, the project direction shifted to a template-based complaint drafting approach. To maintain and enhance the utilization of AI within the service, RAG (Retrieval-Augmented Generation) was integrated to provide legal consultation functionalities.

---

## Key Features Overview

### Initial Goals and Implementation Flow

- **Initial Goal**: Automatic generation of legal complaints using AI  
  - Attempted to integrate GPT models for complaint drafting  
  - Quality was not up to the desired standards

- **Revised Goal**: Utilize AI to generate high-quality complaint templates and then fill in required fields  
  - Achieving a consistent template format purely through LLM models proved challenging  
  - Managing static components like templates through code was more advantageous

After these adjustments, the reliance on AI decreased. To counterbalance this, AI-driven legal consultation features were introduced.

---

### AI Utilization Plan

- **Legal Consultation with RAG Integration**  
  - Use a legal case database and RAG to provide reference information for each incident  
  - Offer estimated settlement amounts and sentencing guidelines  
  - Provide simple legal advice via AI

With this approach, the static template creation is handled through code, while functions such as legal consultation, settlement and sentencing predictions, and case recommendations leverage RAG technology and GPT API integration.

---

## Dataset Composition and Utilization

(Leave this section as is)

### Dataset Example

```json
{
    "id": 1,
    "category": "ì„±ì¶”í–‰",
    "text": "í”¼ê³ ì¸ì€ ì§€í•˜ì²  ë‚´ì—ì„œ í”¼í•´ìì˜ ì‹ ì²´ë¥¼ ê³ ì˜ë¡œ ì ‘ì´‰í•˜ì—¬ ì¶”í–‰í•˜ì˜€ë‹¤.",
    "settlement": 3000000,
    "sentence": 8,
    "verdict_summary": "ê°•ì œì¶”í–‰ì£„ë¡œ ì§•ì—­ 8ê°œì›”, í•©ì˜ê¸ˆ 300ë§Œì› ì„ ê³ .",
    "keywords": ["ëŒ€ì¤‘êµí†µ", "ê°•ì œì¶”í–‰", "ì„±ë²”ì£„"]
},
{
    "id": 1,
    "category": "ì„±í¬ë¡±",
    "text": "í”¼ê³ ì¸ì€ íšŒì‚¬ íšŒì‹ ìë¦¬ì—ì„œ ì—¬ì§ì›ë“¤ì—ê²Œ 'ìˆ ì€ ì˜¤ë¹ ë‘ ë§ˆì…”ì•¼ ì œë§›ì´ì§€'ë¼ëŠ” ë“±ì˜ ì„±ì  ë°œì–¸ì„ ìˆ˜ì°¨ë¡€ í•˜ì˜€ë‹¤.",
    "settlement": 3000000,
    "sentence": 0,
    "verdict_summary": "ì„±í¬ë¡± í˜ì˜ë¡œ ë²Œê¸ˆ 300ë§Œì› ì„ ê³ . íšŒì‚¬ ì§•ê³„ìœ„ì›íšŒì—ì„œ ì •ì§ 3ê°œì›” ì²˜ë¶„.",
    "keywords": ["íšŒì‹", "ì–¸ì–´ì ì„±í¬ë¡±", "ì§ì¥ë‚´ì„±í¬ë¡±"]
},
{
    "id": 1,
    "category": "ì˜¨ë¼ì¸ ìš•ì„¤",
    "text": "í”¼ê³ ì¸ì€ SNSì—ì„œ í”¼í•´ìì˜ ì™¸ëª¨ë¥¼ ì¡°ë¡±í•˜ë©° ì¸ì‹ ê³µê²©ì„± ëŒ“ê¸€ì„ ì§€ì†ì ìœ¼ë¡œ ê²Œì‹œí–ˆë‹¤.",
    "settlement": 150000,
    "sentence": 3,
    "verdict_summary": "SNSì—ì„œ í”¼í•´ìì˜ ì™¸ëª¨ë¥¼ ì¡°ë¡±í•˜ë©° ëª…ì˜ˆí›¼ì† ë° ëª¨ìš• í˜ì˜ë¡œ ì§•ì—­ 3ê°œì›”, í•©ì˜ê¸ˆ 150000ì› ì„ ê³ .",
    "keywords": ["SNS", "ì™¸ëª¨ ë¹„í•˜", "ëª¨ìš•"]
},
{
    "id": 1,
    "category": "ì¤‘ê³ ê±°ë˜ ì‚¬ê¸°",
    "text": "í”¼ê³ ì¸ì€ ê³ ê¸‰ ë ˆì‹œí”¼ ì±…ì„(ë¥¼) ê±°ë˜ í›„ê¸° ì‚¬ì´íŠ¸ë¥¼ ì¡°ì‘í•´ í”¼í•´ìë¥¼ ìœ ì¸í•˜ë©° í”¼í•´ìë¡œë¶€í„° 389000ì›ì„ ì†¡ê¸ˆë°›ê³ , ì´í›„ ì ì í–ˆë‹¤.",
    "settlement": 583039,
    "sentence": 8,
    "verdict_summary": "ê³ ê¸‰ ë ˆì‹œí”¼ ì±… íŒë§¤ ì‚¬ê¸° í˜ì˜ë¡œ ì§•ì—­ 8ê°œì›”, í•©ì˜ê¸ˆ 583039ì› ì„ ê³ .",
    "keywords": ["ê³ ê¸‰ ë ˆì‹œí”¼ ì±…", "ì¤‘ê³ ê±°ë˜", "ì‚¬ê¸°"]
}
```

---

## RAG Technology Overview

RAG (Retrieval-Augmented Generation) combines large language models with external databases (e.g., case law, legal statutes) to improve response accuracy. The model first searches for relevant data (Retrieval), then uses a generative model (Generation) to produce reliable answers based on the retrieved documents.

### Implementation Details

1. **Data Retrieval**:  
   - Embed the case law dataset and store it in FAISS for vector search  
   - Quickly retrieve relevant cases based on similarity to the user query

2. **Augmented Generation**:  
   - Provide the retrieved case summaries as context to the GPT model  
   - Generate a legal complaint draft, estimate settlement amounts and sentencing, and offer simple legal advice  
   - Employ prompt engineering, designating the model as a â€œlawyerâ€ for more authoritative and specialized responses

### Example Workflow

- User Query: "Please find cases applicable to a physical assault incident that took place in Seoulâ€™s Gangnam district on December 1, 2024."
- RAG Process:
  1. Perform a vector similarity search in the case law database for â€œphysical assaultâ€  
  2. Provide the relevant cases to GPT as context  
  3. GPT drafts a complaint and provides legal explanations based on retrieved cases

This ensures the user obtains legally grounded documentation derived from the latest and most relevant case law.

Below is the "Failure Report" section translated into English while maintaining the original structure and format. The dataset section and the rest of the content remain unchanged.

---

## Failure Report

During the development of this project, two specific failure cases were encountered. Documenting these failures aims to help prevent similar issues in the future and to explore more effective solutions.

### Failure Case 1: Technical and Model-Related Limitations in GPT-2 Fine-Tuning

Initially, the plan was to fine-tune GPT-2 to directly produce complaint documents in the desired legal format. To achieve this, we attempted to fine-tune KoGPT-2 (a Korean-adapted GPT-2 model distributed by SKT via gluonnlp) based on the following repository: [KoGPT2-FineTuning](https://github.com/gyunggyung/KoGPT2-FineTuning). However, we encountered the following problems:

- **Technical Constraints (Problem 1)**  
  - Operating environment: GTX-3070Ti, CUDA 12.3  
  - Using PyTorch compatible with CUDA 12.3 required Python 3.8 or higher  
  - KoGPT-2 fine-tuning required MXNet (a C++ library), but the compatible NumPy version for MXNet did not align with Python 3.8  
  This led to complex attempts at resolving library version conflicts, switching GPUs or machines, and other workarounds.

- **Model Characteristics (Problem 2)**  
  - GPT-2 was originally designed as a context expansion model rather than a dialog-based model. While it can generate long text from short prompts, it is difficult to ensure it produces a strictly formatted document (e.g., a legal complaint template).  
  Even with successful fine-tuning, achieving the exact formatting requirements was challenging.

Due to these issues, we abandoned the GPT-2 approach and shifted our focus to fine-tuning GPT-3.5.

### Failure Case 2: Training Failures in the GPT-3.5 Fine-Tuning Environment

GPT-3.5 can be tuned on OpenAIâ€™s Playground without additional code, relying solely on hyperparameters and datasets (prompts). The following steps were taken:

- **Dataset Preparation**: Created a dataset suitable for GPT-3.5 (approximately 500 examples, referenced from the AI_for_dataset repository).
- **Hyperparameter Settings**:  
  1) Learning Rate: Set to 2, aiming to minimize the loss function effectively.  
  2) Epochs: Set to 3, as repeatedly training the entire dataset too many times could lead to overfitting.  
  3) Batch Size: Set to 1 to process the data in the smallest possible increments, ensuring more granular training.

However, after completing the first epoch, OpenAI returned a training execution error. Subsequently, all models under training failed, forcing the abandonment of the project. This outcome was disappointing, and if anyone reading this has insights or suggestions, we encourage them to reach out to a team member via email.
![image](https://github.com/user-attachments/assets/4c3396f0-9f6a-4438-a4b5-40b954e23814)
![image](https://github.com/user-attachments/assets/7c88b516-57cd-4bd3-bef0-3c6139a6b47c)
![image](https://github.com/user-attachments/assets/f2133d90-36f7-448e-a70a-1703bb14bd29)

---


## ğŸ“— Related Documents
[Smart Legal Form Builder PPT](https://github.com/Smart-Legal-Form-Builder/Document/blob/main/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EB%B0%8F%20%EC%9D%91%EC%9A%A9%20ppt_%EC%B5%9C%EC%A2%85.pdf)  

[Smart Legal Form Builder Report](https://github.com/Smart-Legal-Form-Builder/Document/blob/main/Smart%20Legal%20Form%20Builder%20Report.pdf)
